---
title: "High dimensional Bayesian regression with shrinkage priors"
output:
  html_document:
    df_print: paged
---

The setup is high-dimensional linear regression 
using a "shrinkage prior", following 
Carvalho et al., 2009, 2010, Bhadra et al. (2019),
Polson & Scott, 2012, Johndrow et al. (2020),
Biswas et al. (2022).

The data set is `riboflavin` with $p=4088$ covariates.
The test function is $h:x\mapsto x_{2564}$ which corresponds
to a multimodal marginal distribution in the posterior.

### Files to run

(In this order)

- `highdimreg_longrun.R`
- `highdimreg_meetingtimes.R`
- `highdimreg_avar.R`

```{r setup, warning=FALSE, message=FALSE}
library(unbiasedpoisson)
library(dplyr)
library(doParallel)
setmytheme()
## import functions
source("highdimreg_functions.R")
```

### MCMC run

We start by loading MCMC runs, and plotting some traces.

```{r longruntrace, warning=FALSE}
load(file = "../output/highdimreg.longrun.RData")
gtrace <- ggplot(history.df %>% filter(iteration >= 10, iteration <= 1000, chain <= 3),
       aes(x = iteration, y = value, group = chain, colour = factor(chain))) + geom_line() + xlab("iteration") + ylab(TeX("$\\beta_{2564}$")) +
  scale_color_manual(values = c("black", "#005BBB", "#FFD500", "red")) + theme(legend.position = "none")
gtrace
ggsave(filename = '../output/highdimreg.trace.pdf', plot = gtrace, width = 6, height = 5)
```

Let's have a look at the marginal histogram of $h(X)$ under the target distribution $\pi$.
We use a log-scale on the y-axis, which is not traditional, but helps visualizing the two modes:
one at the origin, and one around $-1.5$.

```{r marginalhistogram, warning=FALSE, message=FALSE}
burnin <- 2000
ghist <- ggplot(history.df %>% filter(iteration >= burnin), aes(x = value)) + geom_histogram(aes(y=..density..), binwidth=0.05) + xlab(TeX("$\\beta_{2564}$"))
ghist <- ghist + scale_y_log10() + xlim(-2.5, .5)
ghist
ggsave(filename = '../output/highdimreg.hist.pdf', plot = ghist, width = 6, height = 5)
```

### Convergence to stationarity

We use coupled chains to assess the marginal convergence of the MCMC
algorithm. The coupling is described in Biswas et al. 2022 and we provide
a self-contained description in our paper.

```{r tvupperbounds, message=FALSE, warning=FALSE}
load(file = "../output/highdimreg.meetings.RData")
niterations <- 2000
ubounds <- sapply(1:niterations, function(t) tv_upper_bound(unlist(meetingtimes), lag, t))
g_tvbounds <- qplot(x = 1:niterations, y = ubounds, geom = "line") +
  ylab("TV distance") + xlab("iteration")
g_tvbounds <- g_tvbounds + scale_y_log10(breaks = c(0, 0.001, 0.01, 0.1,1)) + geom_rangeframe() #+ geom_vline(xintercept = 1000, linetype = 2)
# g_tvbounds <- g_tvbounds + geom_hline(yintercept = ubounds[1000], linetype = 2)
g_tvbounds
ggsave(filename = '../output/highdimreg.tvbounds.pdf', plot = g_tvbounds, width = 6, height = 5)
```

### Unbiased asymptotic variance estimation

We next try to estimate $v(P,h)$ using our proposed strategy.

The following defines functions to report confidence intervals
based on the nonparametric bootstrap, for means, variances and "inefficiencies"
which are products of a mean and a variance.

```{r bootstrap_functions}
get_mean_with_booterror <- function(v, nb = 1000){
  bootresult <- boot::boot(data = v, statistic = function(v,indices) mean(v[indices]), R = nb)
  paste0("[", 
         paste0(prettyNum(as.numeric(quantile(bootresult$t, probs = c(0.025, 0.975))), digits=0, scientific = F, nsmall = 0), collapse = " - "),
         "]")
}
# print variance
get_var_with_booterror <- function(v, nb = 1000){
  bootresult <- boot::boot(data = v, statistic = function(v,indices) var(v[indices]), R = nb)
  paste0("[", 
         paste0(prettyNum(as.numeric(quantile(bootresult$t, probs = c(0.025, 0.975))), digits=2, scientific = T), collapse = " - "),
         "]")
}
# print efficiency
get_eff_with_booterror <- function(v, c, nb = 1000){
  bootresult <- boot::boot(data = cbind(v,c), statistic = function(x,indices) var(x[indices,1])*mean(x[indices,2]), R = nb)
  paste0("[", 
         paste0(prettyNum(as.numeric(quantile(bootresult$t, probs = c(0.025, 0.975))), digits=2, scientific = T), collapse = " - "),
         "]")
}
```

Let us now produce a table summarizing the performance
of the proposed method to estimate $v(P,h)$.

```{r highdim_avar}
load(file = "../output/highdimreg.uavar.RData")

results.df <- foreach(irep = 1:nrep, .combine=rbind) %do% {
  run <- results[[irep]]
  data.frame(natoms = natoms_seq,
             rep = irep,
             estimator = run$estimator[1,],
             cost = run$cost,
             pih = mean(run$pih),
             varh = run$varh,
             cost_fishyestimation = run$cost_fishyterms)
}

table_highdimreg <- results.df %>% ungroup() %>% group_by(natoms) %>%
  summarise(pestimate = get_mean_with_booterror(estimator), pcost = get_mean_with_booterror(cost), pfishycost = get_mean_with_booterror(cost_fishyestimation), pvariance = get_var_with_booterror(estimator), pefficiency = get_eff_with_booterror(estimator,cost))
table_highdimreg <- table_highdimreg %>% setNames(c("R", "estimate", "total cost", "fishy cost",
                                        "variance of estimator", "inefficiency"))
print(table_highdimreg, width = Inf)
knitr::kable(table_highdimreg, digits = 0, row.names = NA, format = 'latex', escape = FALSE) %>%
  cat(., file = '../output/highdimreg.summary.tex')
```

This is based on `r nrep` independent runs.
We see that we can fairly accurately estimate $v(P,h)$... basically around 80 here.
The table reports the generated confidence interval for $v(P,h)$,
the total cost per run, the subpart of the cost due to fishy function estimation,
the variance of each run, and the inefficiency (variance times mean cost).
The table also shows the beneficial impact of increasing $R$ (the inefficiency decreases, which
is a good thing).

### Unbiased MCMC tuning

From our runs we can also extract unbiased MCMC estimates of $\pi(h)$, 
and their cost. Out of this we can estimate the expected cost and the variance
of the estimates, and thus the inefficiency of unbiased MCMC defined as the product of the two.

```{r unbiasedmcmc}
unbiasedmcmc.df <- foreach(irep = 1:nrep, .combine=rbind) %do% {
  run <- results[[irep]]
  data.frame(rep = irep,
             estimate_umcmc = mean(run$pih),
             cost_umcmc = run$cost[1] - run$cost_fishyterms[1])
}

unbiasedmcmc.summary.df <- unbiasedmcmc.df %>% summarise(mean_estimate = mean(estimate_umcmc),
                              var_estimate = var(estimate_umcmc),
                              meancost = mean(cost_umcmc))

## compare with long run estimate
# mean(history.df %>% filter(iteration > 2000) %>% pull(value))
#
# variance of each unbiased estimator of pi(h)
2 * unbiasedmcmc.summary.df$var_estimate
# cost of each unbiased estimator of pi(h)
unbiasedmcmc.summary.df$meancost/2
# inefficiency
unbiasedmcmc.summary.df$var_estimate * unbiasedmcmc.summary.df$meancost
```

We can see that the inefficiency is not much higher than the 
asymptotic variance, estimated around `r table_highdimreg$estimate[3]`.
This means that the choice of tuning parameters $k,\ell,L$,
as well as the coupling strategy implemented here,
cannot be much improved, in terms of inefficiency.

Note that in unbiased MCMC, we discard the first $k$ iterations 
of each run, so that a loss of efficiency of at least $k/\ell$ is expected
relative to MCMC. Here $k/\ell$ is equal to `r k/m*100`%.

