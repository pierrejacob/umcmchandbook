---
title: "Cauchy Normal"
output:
  html_document:
    df_print: paged
---

### Files to run

(In this order)

- `cauchynormal_meetingtimes.R`
- `cauchynormal_fishyestimation.R`
- `cauchynormal_avar.R`
- `cauchynormal_avar_optimalxi.R`


### Target 

The target is the posterior distribution in a Cauchy location model,
with a Normal prior on the location parameter denoted by $\theta$.
We observe $n=3$ measurements $z_{1}=-8,z_{2}=+8,z_{3}=+17$, assumed
to be realizations of $\text{Cauchy}(\theta,1)$. The prior on $\theta$
is $\text{Normal}(0,100)$. The target density is shown below.

```{r setup, warning=FALSE, message=FALSE}
library(unbiasedpoisson)
library(tidyverse)
setmytheme()
set.seed(1)
## import functions for the Cauchy-Normal model
source("cauchynormal_functions.R")
## 
targetpdf <- function(z) sapply(z, function(t) exp(unnormalizedlogpdf(t)))
normalizingconstant <- integrate(f = targetpdf,lower = -30, upper = 40)$val
xseq <- seq(from = -30, to = 40, length.out = 1000)
gpi <- qplot(xseq, sapply(xseq, function(z) targetpdf(z)/normalizingconstant), geom = 'line') + xlab('x') + ylab(TeX('$\\pi(x)$'))
gpi
ggsave(filename = "../output/cauchynormal.target.pdf", plot = gpi, width = 8, height = 5)
```

### Algorithms and convergence to stationarity

We consider two MCMC algorithms.

The first is a Gibbs sampler described
in Robert (1995), alternating between Exponential
and Normal draws: 

- $\eta_{i}|\theta \sim\text{Exponential}\left(\frac{1+(\theta-z_{i})^{2}}{2}\right)\quad\forall i=1,\ldots,n,$

- $\theta'|\eta_{1},\ldots,\eta_{n} \sim\text{Normal}\left(\frac{\sum_{i=1}^{n}\eta_{i}z_{i}}{\sum_{i=1}^{n}\eta_{i}+\sigma^{-2}},\frac{1}{\sum_{i=1}^{n}\eta_{i}+\sigma^{-2}}\right)$,

where $\sigma^{2}=100$ is the prior variance. The coupling of this
Gibbs sampler is done using common random numbers for the $\eta$-variables,
and a maximal coupling for the update of $\theta$.

The second algorithm is a Metropolis--Rosenbluth--Teller--Hastings
(MRTH) algorithm with Normal proposal, with standard deviation $10$.
Its coupling employs a reflection-maximal coupling of the proposals.

We first diagnose the marginal convergence of each algorithm,
using couplings to produce upper bounds on the TV distance to stationarity as a function
of time.

```{r meetings, warning=FALSE}
load(file = "../output/cauchynormal.gibbs.meetings.RData")
load(file = "../output/cauchynormal.mrth.meetings.RData")
## two TV upper bounds
niterations <- 150
df <- data.frame(sampler = "Gibbs", t = 1:niterations, tvub = sapply(1:niterations, function(t) tv_upper_bound(unlist(meetingtimes_gibbs), lag, t)))
df <- rbind(df,
      data.frame(sampler = "MRTH", t = 1:niterations, tvub = sapply(1:niterations, function(t) tv_upper_bound(unlist(meetingtimes_mrth), lag, t))))
## plot 
gtv <- ggplot(df, aes(x = t, y = tvub, colour = sampler)) + geom_line() + scale_y_log10() + 
  ylab("TV distance") + xlab("iteration") + scale_color_manual(name = "sampler:", values = c("#005BBB", "#FFD500"))
gtv
ggsave(filename = "../output/cauchynormal.tvupperbounds.pdf", plot = gtv, width = 8, height = 5)
```

The upper bounds might suggest that one sampler converges faster than the other;
however these are only upper bounds, so the actual TV distances might be ordered differently.

### Fishy function estimation

We consider the test function $h:x\mapsto x$. 
The state $y$ used to define the fishy function $g_{y}$ is set to zero. 
The initial distribution, for both chains, is $\pi_{0}=\text{Normal}(0,1)$. 

Next we plot the estimated fishy function associated with each algorithm.

```{r fishyfunction, eval=TRUE}
load(file = "../output/cauchynormal.fishyfunction.RData")
df <- df %>% setNames(c("x", "Gibbs", "MRTH")) %>% pivot_longer(!x, names_to = "sampler", values_to = "estimator")
ghtilde <- ggplot(df, aes(x = x, y = estimator, colour = sampler)) + geom_smooth(formula = y ~ s(x, bs = "cs"), method = "gam") +
  scale_color_manual(name = "sampler:", values = c("#005BBB", "#FFD500"))
ghtilde <- ghtilde + xlab("x") + ylab(TeX("fishy function(x)$"))
print(ghtilde)
ggsave(filename = '../output/cauchynormal.htilde.pdf', plot = ghtilde, width = 8, height = 5)
```

If we interpret the fishy function as an indication of the asymptotic bias of MCMC,
we see that this bias diverges for MRTH whereas it seems uniformly bounded for the Gibbs sampler.

Below we plot the estimated second moment of the estimator,
i.e. $\mathbb{E}[G_y(x)^2]$ for a range of values of $x$. 

```{r secondmoment, eval=TRUE}
ghtildesquare <- ggplot(df, aes(x = x, y = estimator^2, colour = sampler)) + geom_smooth(formula = y ~ s(x, bs = "cs"), method = "gam") +
  scale_color_manual(name = "sampler:", values = c("#005BBB", "#FFD500"))
ghtildesquare <- ghtildesquare + xlab("x") + ylab(TeX("second moment$(x)$"))
print(ghtildesquare)
ggsave(filename = '../output/cauchynormal.Htilde.m2.pdf', plot = ghtildesquare, width = 8, height = 5)
```

### Asymptotic variance estimation

Next we approximate the asymptotic variance $v(P,h)$ 
associated with both algorithms.

We first define some function to construct bootstrap-based $95\%$ confidence
interval for means, variance and inefficiencies. This will be used to produce tables.

```{r functionbootstrap}
get_mean_with_booterror <- function(v, nb = 1000){
  bootresult <- boot::boot(data = v, statistic = function(v,indices) mean(v[indices]), R = nb)
  paste0("[", 
         paste0(prettyNum(as.numeric(quantile(bootresult$t, probs = c(0.025, 0.975))), digits=0, scientific = F, nsmall = 0), collapse = " - "),
         "]")
}
# print variance
get_var_with_booterror <- function(v, nb = 1000){
  bootresult <- boot::boot(data = v, statistic = function(v,indices) var(v[indices]), R = nb)
  paste0("[", 
         paste0(prettyNum(as.numeric(quantile(bootresult$t, probs = c(0.025, 0.975))), digits=2, scientific = T), collapse = " - "),
         "]")
}
# print efficiency
get_eff_with_booterror <- function(v, c, nb = 1000){
  bootresult <- boot::boot(data = cbind(v,c), statistic = function(x,indices) var(x[indices,1])*mean(x[indices,2]), R = nb)
  paste0("[", 
         paste0(prettyNum(as.numeric(quantile(bootresult$t, probs = c(0.025, 0.975))), digits=2, scientific = T), collapse = " - "),
         "]")
}
```

#### Gibbs sampler

For the Gibbs sampler we obtain the following results.

```{r gibbs_avar}
load(file = "../output/cauchynormal.gibbs.avar.RData")
table_gibbs <- results.gibbs %>% ungroup() %>% group_by(natoms) %>%
  summarise(pestimate = get_mean_with_booterror(estimator), pcost = get_mean_with_booterror(cost), pfishycost = get_mean_with_booterror(cost_fishyestimation), pvariance = get_var_with_booterror(estimator), pefficiency = get_eff_with_booterror(estimator,cost))
table_gibbs <- table_gibbs %>% setNames(c("R", "estimate", "total cost", "fishy cost", 
                                        "variance of estimator", "inefficiency"))
print(table_gibbs, width = Inf)
knitr::kable(table_gibbs, digits = 0, row.names = NA, format = 'latex', escape = FALSE) %>%
  cat(., file = '../output/cauchynormal.gibbs.summary.tex')
```

That table is obtained using `r nrep` independent replicates. From the estimates we obtain confidence intervals for each quantity in the table using the nonparametric bootstrap (i.e. sample with replacement and then compute aggregate quantity). The columns are:

- estimate: overall estimate of $v(P,h)$, obtained by averaging `r nrep` independent estimates
- cost: total cost of each proposed estimate, in units of "Markov transitions" 
- fishy cost: subcost associated with the fishy estimates (increases with $R$)
- variance of estimator: empirical variance of the estimates (decreases with $R$)
- inefficiency: product of the variance and the total cost (smaller is better)

We see that the variance decreases with $R$ but the cost increases. The inefficiency seems to "stabilize" as $R$ gets bigger. An important take-away is that $R=1$ is not a good choice. 

As a rule of thumb we propose to choose $R$ such that "fishy cost" is around $50\%$ of the total cost: this way we waste at most half of our computing time.

Here we conclude that $v(P,h)$ for the Gibbs sampler is between $850$ and $900$ with large probability.

####

Next we consider the MRTH strategy. We produce the same type of table.

```{r mrth_avar}
load(file = "../output/cauchynormal.mrth.avar.RData")

table_mrth <- results.mrth %>% ungroup() %>% group_by(natoms) %>%
  summarise(pestimate = get_mean_with_booterror(estimator), pcost = get_mean_with_booterror(cost), pfishycost = get_mean_with_booterror(cost_fishyestimation), pvariance = get_var_with_booterror(estimator), pefficiency = get_eff_with_booterror(estimator,cost))
table_mrth <- table_mrth %>% setNames(c("R", "estimate", "total cost", "fishy cost", 
                                      "variance of estimator", "inefficiency"))
print(table_mrth, width = Inf)
knitr::kable(table_mrth, digits = 0, row.names = NA, format = 'latex', escape = FALSE) %>%
  cat(., file = '../output/cauchynormal.mrth.summary.tex')
```

Here we find that $v(P,h)$ is between $330$ and $350$ with large probability.
Thus we can be confident that the asymptotic variance is smaller for MRTH than 
for the Gibbs sampler.

### Choice of selection probabilities

The above results are obtained with $\xi = 1/N$, i.e. uniform 
probability of the choice of atom at which to estimate the fishy function.
We can optimize these probabilities in order to reduce the variance
of the proposed estimator. Here we do so while keeping $R=10$ fixed.

```{r optimalselection}
## load results for Gibbs
load(file = "../output/cauchynormal.gibbs.avar.opt.RData")
table_gibbs <- results.gibbs %>% filter(natoms == 10) %>% ungroup() %>% 
  summarise(algo = "Gibbs",  selection = "uniform", pestimate = get_mean_with_booterror(estimator), pcost = get_mean_with_booterror(cost), pfishycost = get_mean_with_booterror(cost_fishyestimation), pvariance = get_var_with_booterror(estimator), pefficiency = get_eff_with_booterror(estimator,cost))
table_gibbs.opt <- results.gibbs.opt %>% ungroup() %>% 
  summarise(algo = "Gibbs", selection = "optimal", pestimate = get_mean_with_booterror(estimator), pcost = get_mean_with_booterror(cost), pfishycost = get_mean_with_booterror(cost_fishyestimation), pvariance = get_var_with_booterror(estimator), pefficiency = get_eff_with_booterror(estimator,cost))

## load results for MRTH
load(file = "../output/cauchynormal.mrth.avar.opt.RData")
table_mrth.opt <- results.mrth.opt %>% ungroup() %>% 
  summarise(algo = "MRTH", selection = "optimal", pestimate = get_mean_with_booterror(estimator), pcost = get_mean_with_booterror(cost), pfishycost = get_mean_with_booterror(cost_fishyestimation), pvariance = get_var_with_booterror(estimator), pefficiency = get_eff_with_booterror(estimator,cost))
table_mrth <- results.mrth %>% filter(natoms == 10) %>% ungroup() %>% 
  summarise(algo = "MRTH",  selection = "uniform", pestimate = get_mean_with_booterror(estimator), pcost = get_mean_with_booterror(cost), pfishycost = get_mean_with_booterror(cost_fishyestimation), pvariance = get_var_with_booterror(estimator), pefficiency = get_eff_with_booterror(estimator,cost))
table_selectionprob <- rbind(table_gibbs, table_gibbs.opt, table_mrth, table_mrth.opt)
table_selectionprob <- table_selectionprob %>% setNames(c("algorithm", "selection $\\xi$", "estimate", "total cost", "fishy cost", 
                                        "variance of estimator", "inefficiency"))
print(table_selectionprob[,-c(3,4)])
knitr::kable(table_selectionprob[,-c(3,4)], digits = 0, row.names = NA, format = 'latex', escape = FALSE) %>%
  cat(., file = '../output/cauchynormal.selectionprob.tex')

```


We find that the optimal selection probabilities decrease the variance,
as they should. The effect is not drastic either; we expect the effect to be larger
for smaller values of $R$.

Note also that the cost associated with fishy function is impacted, because
the atoms are selected according to the second moment of the fishy function estimator,
which is related to the meeting times and thus the cost. Interestingly here,
the optimal $\xi$ leads to an increase in cost for the Gibbs sampler and
a decrease in cost for MRTH.





