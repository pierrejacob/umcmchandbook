---
title: 'tutorial: main functions'
output: html_document
---

```{r setup1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This file describes how to use some of the main functions implemented
in this package. Specifically, the main novelty is in the functions:
- `sample_unbiasedfishy`
- `sample_unbiasedvar_reservoir`


## AR(1) model

### Definition

We consider an AR(1) model defined as
$$ X_t = \phi X_{t-1} + W_t,$$
with $W_t$ i.i.d. $\text{Normal}(0,1^2)$, and $X_{0}\sim \text{Normal}(0,4^2)$. We consider the test function $h:x\mapsto 1(x>1)$. The invariant distribution $\pi$ is $\text{Normal}(0,1/(1-\phi^2))$.

### Goal 

In the following we show how coupled chains can be employed to estimate the solution of the Poisson equation:
$$ \tilde{h} - P\tilde{h} = h - \pi(h),$$
given by 
$$ \tilde{h}:x \mapsto \sum_{t\geq 0} \{P^t h(x) - P^t h(y)\},$$
where we define $y$ as a fixed value, $-2$, below.

Afterward we look at how we can estimate the asymptotic variance $v(P,h)$ in the Central Limit Theorem (CLT):
$$ \sqrt{t}\left(t^{-1}\sum_{t=0}^{t-1} h(X_t) - \pi(h)\right) \longrightarrow_{t\to\infty} \text{Normal}(0,v(P,h)).$$

### Implementation

Let's start by loading the package, and a few other.

```{r setup, message=F, warning=F}
rm(list = ls())
library(unbiasedpoisson)
library(doParallel)
library(doRNG)
library(dplyr)
library(ggplot2)
# set a particular graphical theme
setmytheme()
# register parallel cores
registerDoParallel(cores = detectCores()-2)
# set RNG seed
set.seed(1)
```

We implement the AR(1) model and a coupling of it.

```{r}
# autoregressive parameter
phi <- 0.9
## Markov kernel
## takes a "state" and returns a "state
single_kernel <- function(state){
  return(list(position = phi * state$position + rnorm(1, 0, 1)))
}
## Coupled Markov kernel
## using "reflection-maximal coupling" implemented in the function 'rnorm_reflectionmax'
## takes a pair of "states"
## and returns a pair of "states", and an indicator of state1==state2
coupled_kernel <- function(state1, state2){
  nextvalues <- unbiasedpoisson::rnorm_reflectionmax(mu1 = phi * state1$position, 
                                                     mu2 = phi * state2$position, 
                                                     sigma = 1)
  return(list(state1 = list(position = nextvalues$xy[1]), 
              state2 = list(position = nextvalues$xy[2]), 
              identical = nextvalues$identical))
}

## initial distribution Normal(0, 4^2)
rinit <- function(position){
  if (missing(position)){
    position <- rnorm(1, mean = 0, sd = 4)
  }
  return(list(position = position))
}

## test function x \mapsto 1(x>1)
h <- function(x) (x > 1)
## arbitrary point 'y' used in the definition of the fishy function
y <- -2
```

What is the reflection-maximal coupling mentioned above?
If we have two variables
$X\sim \text{Normal}(\mu_1,\sigma^2)$ and
$Y\sim \text{Normal}(\mu_2,\sigma^2)$ then the reflection-maximal coupling is the following procedure:

- Let $z=\sigma^{-1}(\mu_{1}-\mu_{2})$.

- Sample $\dot{X}\sim\mathcal{N}(0,1)$, and $W\sim\text{Uniform}(0,1)$.

- Set $X = \mu_1 + \sigma \dot{X}$.

- If $W\leq \varphi(\dot{X}+z)/\varphi(\dot{X})$, set $Y=X$;
      else set $Y=\mu_2-\sigma \dot{X}$.

- Return $(X,Y)$.

The generated pair $(X,Y)$ satisfies the marginal distributions
but is also such that $\{X=Y\}$ occurs with maximal probability.

## Meeting times

We can now run lagged chains and see how long they take to meet.
The following code generates meeting times
and plots the survival function, with the y-axis on log scale.

```{r meetings, message=FALSE, warning=FALSE}
## number of independent repeats
nrep <- 1e3
## choice of lag
lag <- 1
## sample i.i.d. meeting times
meetingtime_runs <- foreach (irep = 1:nrep) %dorng% {
  sample_meetingtime(single_kernel, coupled_kernel, rinit, lag = lag)
}
meetingtimes <- sapply(meetingtime_runs, function(x) x$meetingtime)

ecdf_f <- ecdf(meetingtimes-lag)
g_meetings <- ggplot(data=NULL) + stat_function(fun=function(x) 1-ecdf_f(x), n = 1000) + scale_y_log10() + scale_x_continuous(limits = c(1, 50)) + xlab(TeX("time")) + ylab(TeX("$P(\\tau>t)$"))
print(g_meetings)
```

The meeting times provide information on the distance between the chain
and the stationary distributions. Relying on the approach described in

- Jacob, O'Leary, Atchad√©, Unbiased Markov chain Monte Carlo methods with couplings, 2020.
- Biswas, Jacob and Vanetti, Estimating convergence of Markov chains with L-lag couplings, 2019.

we obtain the following plot.

```{r tvupperbounds, message=F, warning=F}
niterations <- 100
ubounds <- sapply(1:niterations, function(t) tv_upper_bound(meetingtimes, lag, t))
g_tvbounds <- qplot(x = 1:niterations, y = ubounds, geom = "line") + 
  ylab("TV distance") + xlab("time") + scale_y_log10()
g_tvbounds
```

The above suggests setting a lag $L=50$ and 
re-doing the computation.

```{r biggerlag}
## choice of lag
lag <- 50
## sample i.i.d. meeting times
meetingtime_biggerlag_runs <- foreach (irep = 1:nrep) %dorng% {
  sample_meetingtime(single_kernel, coupled_kernel, rinit, lag = lag)
}
meetingtime_biggerlag <- sapply(meetingtime_biggerlag_runs, function(x) x$meetingtime)
ubounds_biggerlag <- sapply(1:niterations, function(t) tv_upper_bound(meetingtime_biggerlag, lag, t))
g_tvbounds <- qplot(x = 1:niterations, y = ubounds, geom = "line", colour = "L=1") + 
  ylab("TV distance") + xlab("time") + scale_y_log10()
g_tvbounds <- g_tvbounds + geom_line(aes(y = ubounds_biggerlag, colour = "L=50")) +
  scale_color_manual(name = "", values = c("#005BBB", "#FFD500")) + theme(legend.position = "bottom")
g_tvbounds
```

We see that increasing the lag makes the upper bound on the TV distance tighter.
Generally, increasing the lag leads to more accurate estimates but at a computing cost.

## Fishy function estimation

We set a grid of values $x \in (-5,+5)$
and for each value, we generate unbiased estimators of the fishy function associated
with $h:x\mapsto 1(x>1)$.

```{r fishyestimation, message=F, warning=F}
## obtain 'nrep' unbiased estimators for a grid of value of x
nrep <- 1e2
## grid of values
xseq <- seq(from = -5, to = 5, length.out = 25)
df <- data.frame()
state_y <- rinit(y)
for (x in xseq){
  state_x <- rinit(x)
  ## sample estimators of fishy function at x
  res_ <- foreach(irep = 1:nrep) %dorng% {
    sample_unbiasedfishy(coupled_kernel, h, state_x, state_y)
  }
  df <- rbind(df, data.frame(x = x, estimator = sapply(res_, function(x) x$estimator)))
}
## plot estimated fishy function
ghtilde <- ggplot(df, aes(x = x, y = estimator)) + geom_hline(yintercept = 0, linetype = 1, alpha = 0.2) +
  geom_vline(xintercept = y, linetype = 1) + 
  geom_smooth(colour = "black", method = "gam", formula = y ~ s(x, bs = "cs")) + xlab('x') + ylab("fishy function") + scale_x_continuous(breaks = 2*(-3:3))
ghtilde
```

What happens if we randomize $y$ instead of fixing it to zero?
We can estimate the associated fishy function as follows.

```{r fishyestimation2, message=F, warning=F, cache=T}
## obtain 'nrep' unbiased estimators for a grid of value of x
df2 <- data.frame()
for (x in xseq){
  # set x in the grid
  state_x <- rinit(x)
  res_ <- foreach(irep = 1:nrep, .combine = c) %dorng% {
    # randomize y in each repeat
    state_y <- rinit()
    # set meeting time to infinity
    meetingtime <- Inf
    # keep track of time
    time <- 0
    # estimator starts with h(X_0) - h(Y_0)
    estimator_ <- h(state_x$position) - h(state_y$position)
    # run until the meeting time
    while (is.infinite(meetingtime)){
      time <- time + 1
      # sample new states using coupled kernel
      coupledstates <- coupled_kernel(state_x, state_y)
      state_x <- coupledstates$state1
      state_y <- coupledstates$state2
      # update estimator with increment: h(X_t) - H(Y_t)
      estimator_ <- estimator_ + h(state_x$position) - h(state_y$position)
      # stop if meeting occurred
      if (coupledstates$identical) meetingtime <- time
    }
    # return estimator
    estimator_
  }
  df2 <- rbind(df2, data.frame(x = x, estimator = res_))
}
## plot solution of Poisson equation associated with h
## plot estimated fishy function
ghtilde <- ggplot(df, aes(x = x, y = estimator, colour = "fixed y")) + geom_hline(yintercept = 0, linetype = 1, alpha = 0.2) +
  geom_vline(xintercept = y, linetype = 1) + 
  geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs")) + xlab('x') + ylab("fishy function") + scale_x_continuous(breaks = 2*(-3:3))
ghtilde + geom_smooth(data = df2, aes(colour = "random y"), method = "gam", formula = y ~ s(x, bs = "cs")) + scale_color_manual(name = "", values = c("#005BBB", "#FFD500"))
```

We see that the two fishy functions seem to be equal up to
an additive constant, which is expected from Lemma 21.2.2
in 

- Douc, Moulines, Priouret and Soulier, Markov chains, 2018.

## Asymptotic variance estimation

We are interested in the asymptotic variance in the CLT. The package provides the function
`sample_unbiasedvar_reservoir` to generate unbiased estimators
of that asymptotic variance.

```{r avar}
## set k = lag L as a default
k <- lag
## m = 5 times k so that the proportion of discarded iterates is 1/5
m <- 5*k
## number of independent repeats
nrep <- 1e3
## number of fishy function evals per signed measure
natoms <- 20
## generate i.i.d. unbiased estimator of v(P,h)
uavar <-  foreach(irep = 1:nrep) %dorng% {
  sample_unbiasedvar_reservoir(single_kernel, coupled_kernel, rinit, h = h, k = k, m = m, lag = lag, x_0 = y, natoms = natoms)
}

results <- data.frame(rep = 1:nrep,
                      estimates = sapply(uavar, function(x) x$estimator[1,]),
                      cost = sapply(uavar, function(x) x$cost),
                      costfishy = sapply(uavar, function(x) x$cost_fishyterms))

cat("asymptotic variance:", format(mean(results$estimates), digits = 3),"\n",
"+ or -", format(1.96*sd(results$estimates)/sqrt(nrep), digits = 3)," with 95% probability,\n",
"based on", nrep, "independent runs of the proposed estimators.\n")
```

We can plot the individual estimates
```{r natoms}
ggplot(results, aes(x = estimates)) + geom_histogram(binwidth = 0.25)
```

We see that some of the estimates are negative, even though the estimand (a variance)
is obviously positive. Nevertheless the estimators are unbiased and here they 
yield a fairly precise overall results, with a width of a $95\%$-confidence interval
of `r format(2*1.96*sd(results$estimates)/sqrt(nrep), digits = 3)`.


