---
title: "AR-Binomial SSM experiments"
output:
  html_document:
    df_print: paged
---

The setup is Bayesian inference in a state space model inspired by a neuroscience setting.
The setup is taken from "Unbiased Markov chain Monte Carlo for intractable target distributions", Middleton, Deligiannidis, Doucet & Jacob, 2020, Section 4.2 "Neuroscience experiment".
The MCMC algorithm is particle MCMC (Andrieu et al. 2010).
This produces polynomially ergodic chains, with a performance that depends 
(among other things) on the number
of particles employed at each step of PMCMC.

### Files to run

(In this order)

- `binomialssm1d_longrun.R`
- `binomialssm1d_meetingtimesP64.R`
- `binomialssm1d_meetingtimesP256.R`
- `binomialssm1d_avar.R`

### Data

```{r setup, warning=FALSE, message=FALSE}
rm(list = ls())
library(unbiasedpoisson)
library(dplyr)
library(doParallel)
setmytheme()
```

Plot a subset of the data. For simplicity we consider a time series of length $100$.

```{r data}
re <- read.csv("thaldata.csv", header = FALSE, col.names = FALSE)
observations <- matrix(re[1:3000,1], ncol = 1)
datalength <- nrow(observations)
## subset data for faster computation

subobs <- observations[floor(seq.int(from = 1000, to = 1500, length.out = 100)),,drop=F]
gdata <- qplot(x = 1:length(subobs), y = subobs, geom = "line") + xlab("time") + ylab("response")
gdata
ggsave(plot = gdata, filename = "../output/binomialssm1d.data.pdf", width = 8, height = 5)
observations <- subobs
datalength <- length(observations)
```

State space model is specified as follows. The latent process is AR(1):
$$ x_0 \sim \text{Normal}(0,1),\quad \text{and} \quad \forall t \geq 1\quad x_t | x_{t-1} \sim \text{Normal}(\alpha x_{t-1}, \sigma^2).$$
The observation is Binomial, with a logistic link,
$$ y_t | x_t \sim \text{Binomial}(50, \text{logistic}(x_t)),$$
with $\text{logistic}:x\mapsto 1/(1+\exp(-x))$. The prior is $\text{Uniform}(0,1)$ on $\alpha$
and $\sigma^2$ is fixed to 1.5.

We estimate the likelihood using Controlled SMC (Heng et al. 2020), using 3 iterations of Controlled SMC at each particle MCMC step, and the number of particles
employed is denoted by $P$. We initialize the chains by sampling $\alpha$
from its prior distribution $\text{Uniform}(0,1)$.

We use a random walk proposal on the parameter space. Here the parameter is one-dimensional,
and at each step we generate a standard deviation from a Uniform distribution on $(0.001, 0.2)$,
and then a Normal noise with that standard deviation. The details are in `binomialssm1d_functions.R`.

### Preliminary MCMC runs

We first run a few independent chains, using $P=256$ particles
in each run of the particle filter. The trace plot looks as follows.

```{r traceplot, warning=FALSE, message=FALSE}
load(file = "../output/binomialssm1d.longrun.RData")
ggplot(history %>% filter(iteration < 3e2), aes(x = iteration, y = x1, group = chain)) + geom_line() + ylab(TeX("$\\alpha$"))
```

The histogram of $\alpha$ looks as follows.

```{r histogram, warning=FALSE, message=FALSE}
gposterior <- ggplot(history %>% filter(iteration >= 1e3), aes(x = x1)) + geom_histogram(binwidth=0.001) + xlab(TeX("$\\alpha$"))
gposterior
ggsave(plot = gposterior, filename = "../output/binomialssm1d.posterior.pdf", width = 8, height = 5)

```

So the posterior distribution is very concentrated on the right boundary of $[0,1]$,
the support of the (uniform) prior on $\alpha$.

### Meeting times

We couple the chains using a reflection-maximal coupling of the proposals.
We generate meeting times, using two numbers of particles: $P=64$ and $P=256$.
We know from Middleton et al., 2020 that this will have an effect on the tails
of the distribution of the meeting time.

#### With 64 particles

We plot the empirical survival function of the meeting times
obtained when $P=64$.

```{r survival_meetingtimesP64, warning=FALSE, message=FALSE}
load(file = "../output/binomialssm1d.meetings.P64.RData")
ecdf_f <- ecdf(meetingtimes-lag)
g_meetings64 <- ggplot(data=NULL) + stat_function(fun=function(x) 1-ecdf_f(x), n = 1000) + scale_y_log10()+scale_x_log10(limits = c(1, 5e3)) + xlab(TeX("$t$")) + ylab(TeX("$P(\\tau>t)$"))
print(g_meetings64)
ggsave(filename = '../output/binomialssm1d.meetings64survival.pdf', plot = g_meetings64, width = 8, height = 5)
```


The plot above uses logarithmic scales for both $t$ and $\mathbb{P}(\tau > t)$,
so that a straight line indicates a polynomial decay. We see a straight line
on the right hand side of the plot, indicating polynomial tails for $\tau$.
Using a linear regression we estimate the polynomial rate.

```{r survival_meetingtimesP64reg}
## what's the slope on the right hand side?
x <- seq(from = 200, to = 5000, by = 1)
y <- 1-ecdf_f(x)
lmres <- lm(log(y) ~ log(x))
ggplot(data=data.frame(logx=log(x),logy=log(y)), aes(x = logx, y = logy)) + geom_line() +
  geom_abline(intercept = lmres$coefficients[1], slope = lmres$coefficients[2], colour = 'red') + xlab(TeX("$\\log(t)$")) + ylab(TeX("$\\log P(\\tau>t)$"))
```

```{r regcoeffP64}
print(lmres)
```

The polynomial rate is estimated around `r round(lmres$coefficients[2], digits=3)`.
This corresponds to heavy tails: it suggests that $\tau$ might barely have a finite expectation,
and probably an infinite variance.

#### With 256 particles

We plot the empirical survival function of the meeting times
obtained when $P=256$.

```{r survival_meetingtimesP256, warning=FALSE, message=FALSE}
load(file = "../output/binomialssm1d.meetings.P256.RData")
ecdf_f <- ecdf(meetingtimes-lag)
g_meetings256 <- ggplot(data=NULL) + stat_function(fun=function(x) 1-ecdf_f(x), n = 1000) + scale_y_log10()+scale_x_log10(limits = c(1, 4e2)) + xlab(TeX("$t$")) + ylab(TeX("$P(\\tau>t)$"))
print(g_meetings256)
ggsave(filename = '../output/binomialssm1d.meetings256survival.pdf', plot = g_meetings256, width = 8, height = 5)
```
Using a linear regression we estimate the polynomial rate.

```{r survival_meetingtimesP256reg}
## what's the slope on the right hand side?
x <- seq(from = 100, to = 298, by = 1)
y <- 1-ecdf_f(x)
lmresP256 <- lm(log(y) ~ log(x))
ggplot(data=data.frame(logx=log(x),logy=log(y)), aes(x = logx, y = logy)) + geom_line() +
  geom_abline(intercept = lmresP256$coefficients[1], slope = lmresP256$coefficients[2], colour = 'red') + xlab(TeX("$\\log(t)$")) + ylab(TeX("$\\log P(\\tau>t)$"))
```
```{r regcoeffP256}
print(lmresP256)
```

The polynomial rate is estimated around `r round(lmresP256$coefficients[2], digits=3)`.
This is less heavy. The variance of $\tau$ should be finite. In the following we thus keep using $P=256$.

### TV upper bounds using couplings

Instead of plotting the survival function of the meeting times,
we can obtain upper bounds on the TV distance, which are perhaps easier to interpret
in terms of the convergence of the marginal chain.

```{r tvupperboundsP256, warning=FALSE, message=FALSE}
load(file = "../output/binomialssm1d.meetings.P256.RData")
niterations <- 200
ubounds <- sapply(1:niterations, function(t) tv_upper_bound(unlist(meetingtimes), lag, t))
g_tvbounds <- qplot(x = 1:niterations, y = ubounds, geom = "line") +
  ylab("TV distance") + xlab("iteration")
g_tvbounds <- g_tvbounds + scale_y_log10(breaks = c(0, 0.001, 0.01, 0.1,1)) + geom_rangeframe()
g_tvbounds
ggsave(filename = '../output/binomialssm1d.tvbounds.P256.pdf', plot = g_tvbounds, width = 6, height = 5)
```


### Unbiased estimator of asymptotic variance

We use $k=500$, $L=500$ and $\ell = 5k$ for unbiased MCMC approximations,
and $R=50$ atoms. We set the test function to $h:\alpha\mapsto \alpha$.

We consider two values for $y$, the location of the state space
employed to define the fishy function$ $g_y$ and its estimator $G_y$.

#### Bad choice of y

First we choose $y = 0.5$, which is quite far 
from where the posterior distribution puts 
most of its mass (i.e. the interval $(0.9,1)).

```{r unbiasedavarP256_badanchor}
load("../output/binomialssm1d.uavar.P256.badanchor.RData")
print(nrep)
print(x_0)
```

We run `r nrep` independent estimates, and for each 
we record the selected atoms and the fishy function estimates at these atoms.
Out of these we estimate the fishy function and plot it.

```{r unbiasedavarP256_badanchor_fishyfunction}
results <- results.badx0
alldf_ <- foreach (irep = 1:nrep, .combine = rbind) %do% {
  selectedatoms_ <- c(results[[irep]][[1]]$selectedatoms, results[[irep]][[2]]$selectedatoms)
  data.frame(irep = irep, atom = sapply(selectedatoms_, function(x) x$position),
             targetpdfestimate = sapply(selectedatoms_, function(x) x$current_target),
             fishyestimate = c(results[[irep]][[1]]$htilde_at_atoms[1,], results[[irep]][[2]]$htilde_at_atoms[1,]),
             meetingtime = c(results[[irep]][[1]]$cost_fishyestimation, results[[irep]][[2]]$cost_fishyestimation))
}
ghtilde <- ggplot(alldf_, aes(x = atom, y = fishyestimate)) + xlab(TeX("$\\alpha$")) +
  geom_smooth(formula = y ~ s(x, bs = "cs"), method = 'gam') + ylab(TeX("fishy function(x)$")) 
ghtilde
ggsave(filename = "../output/binomialssm1d.htilde.badanchor.pdf", plot = ghtilde, width = 8, height = 5)
```

We then show the histogram of the proposed estimator (SUAVE) of $v(P,h)$.

```{r unbiasedavarP256_badanchor_histogramR50}
res_df_badanchor <- foreach (irep = 1:nrep, .combine = rbind) %do% {
  res_ <- results.badx0[[irep]]
  run <- unbiasedvar_from_tworuns(res_[[1]], res_[[2]], 
                                               natoms = natoms_seq)
  data.frame(natoms = natoms_seq,
             rep = irep,
             estimator = run$estimator[1,],
             cost = run$cost,
             pih = mean(run$pih),
             varh = run$varh,
             cost_fishyestimation = run$cost_fishyterms)
}
res_df_badanchorR50 <- res_df_badanchor %>% filter(natoms == 50)
summary_badanchorR50 <- res_df_badanchorR50 %>% summarise(meanestimator = mean(estimator), sdestimator = sd(estimator), varestimator = var(estimator))
ci_low_badanchor <- summary_badanchorR50$meanestimator - 1.96*summary_badanchorR50$sdestimator/sqrt(nrep)
ci_high_badanchor <- summary_badanchorR50$meanestimator + 1.96*summary_badanchorR50$sdestimator/sqrt(nrep)
g_hist_badanchor <- ggplot(res_df_badanchorR50, aes(x = estimator)) + geom_histogram() 
g_hist_badanchor <- g_hist_badanchor + xlab(TeX("estimator of $v(P,h)$"))
g_hist_badanchor
print(summary_badanchorR50$varestimator)
ggsave(plot = g_hist_badanchor, filename = "../output/binomialssm1d.estimR50badanchor.pdf",
       width = 8, height = 5)
```

We see that the variance of the estimator 
is relatively large, e.g. a lot of estimators take negative values.
Using the CLT we obtain a $95\%$ confidence interval (`r round(c(ci_low_badanchor, ci_high_badanchor), digits = 4)`) that makes sense (its boundaries are both positive) but that is not very
precise in relative terms. One option would be to generate many more independent repeats.
But there is another, cheaper option.

#### Better choice of y

Next we choose $y = 0.975$.

```{r unbiasedavarP256_goodanchor}
load("../output/binomialssm1d.uavar.P256.goodanchor.RData")
print(nrep)
print(x_0)
```

Then we produce the same type of plots.

```{r unbiasedavarP256_goodanchor_fishyfunction}
results <- results.goodx0
alldf_ <- foreach (irep = 1:nrep, .combine = rbind) %do% {
  selectedatoms_ <- c(results[[irep]][[1]]$selectedatoms, results[[irep]][[2]]$selectedatoms)
  data.frame(irep = irep, atom = sapply(selectedatoms_, function(x) x$position),
             targetpdfestimate = sapply(selectedatoms_, function(x) x$current_target),
             fishyestimate = c(results[[irep]][[1]]$htilde_at_atoms[1,], results[[irep]][[2]]$htilde_at_atoms[1,]),
             meetingtime = c(results[[irep]][[1]]$cost_fishyestimation, results[[irep]][[2]]$cost_fishyestimation))
}
ghtilde <- ggplot(alldf_, aes(x = atom, y = fishyestimate)) + xlab(TeX("$\\alpha$")) +
  geom_smooth(formula = y ~ s(x, bs = "cs"), method = 'gam') + ylab(TeX("fishy function(x)$")) 
ghtilde
ggsave(filename = "../output/binomialssm1d.htilde.goodanchor.pdf", plot = ghtilde, width = 8, height = 5)
```

The fishy function takes values closer to zero and its estimation error appears to be smaller
(the grey ribbon is narrower around the blue line).
We show the histogram of the proposed estimator (SUAVE) of $v(P,h)$.

```{r unbiasedavarP256_goodanchor_histogramR50}
res_df_goodanchor <- foreach (irep = 1:nrep, .combine = rbind) %do% {
  res_ <- results.goodx0[[irep]]
  run <- unbiasedvar_from_tworuns(res_[[1]], res_[[2]], 
                                               natoms = natoms_seq)
  data.frame(natoms = natoms_seq,
             rep = irep,
             estimator = run$estimator[1,],
             cost = run$cost,
             pih = mean(run$pih),
             varh = run$varh,
             cost_fishyestimation = run$cost_fishyterms)
}
res_df_goodanchorR50 <- res_df_goodanchor %>% filter(natoms == 50)
summary_goodanchorR50 <- res_df_goodanchorR50 %>% summarise(meanestimator = mean(estimator), sdestimator = sd(estimator), varestimator = var(estimator))
ci_low_goodanchor <- summary_goodanchorR50$meanestimator - 1.96*summary_goodanchorR50$sdestimator/sqrt(nrep)
ci_high_goodanchor <- summary_goodanchorR50$meanestimator + 1.96*summary_goodanchorR50$sdestimator/sqrt(nrep)
g_hist_goodanchor <- ggplot(res_df_goodanchorR50, aes(x = estimator)) + geom_histogram() 
g_hist_goodanchor <- g_hist_goodanchor + xlab(TeX("estimator of $v(P,h)$"))
g_hist_goodanchor
print(summary_goodanchorR50$varestimator)
ggsave(plot = g_hist_goodanchor, filename = "../output/binomialssm1d.estimR50goodanchor.pdf",
       width = 8, height = 5)
```

The variance is much reduced.
Using the CLT we obtain a $95\%$ confidence interval (`r round(c(ci_low_goodanchor, ci_high_goodanchor), digits = 5)`), to be compared with what we had when using $y=0.5$: 
 (`r round(c(ci_low_badanchor, ci_high_badanchor), digits = 4)`).

We finally produce a little table summarizing the impact of $y$.

```{r tablesummary}
get_mean_with_booterror <- function(v, nb = 1000){
  bootresult <- boot::boot(data = v, statistic = function(v,indices) mean(v[indices]), R = nb)
  paste0("[", 
         paste0(prettyNum(as.numeric(quantile(bootresult$t, probs = c(0.025, 0.975))), digits=3, scientific = T, nsmall = 0), collapse = " - "),
         "]")
}
# print variance
get_var_with_booterror <- function(v, nb = 1000){
  bootresult <- boot::boot(data = v, statistic = function(v,indices) var(v[indices]), R = nb)
  paste0("[", 
         paste0(prettyNum(as.numeric(quantile(bootresult$t, probs = c(0.025, 0.975))), digits=2, scientific = T), collapse = " - "),
         "]")
}
# print efficiency
get_eff_with_booterror <- function(v, c, nb = 1000){
  bootresult <- boot::boot(data = cbind(v,c), statistic = function(x,indices) var(x[indices,1])*mean(x[indices,2]), R = nb)
  paste0("[", 
         paste0(prettyNum(as.numeric(quantile(bootresult$t, probs = c(0.025, 0.975))), digits=2, scientific = T), collapse = " - "),
         "]")
}

table_badanchor <- res_df_badanchor %>% filter(natoms == 50) %>% 
  summarise(y = "0.5", pestimate = get_mean_with_booterror(estimator), pcost = get_mean_with_booterror(cost), pfishycost = get_mean_with_booterror(cost_fishyestimation), pvariance = get_var_with_booterror(estimator), pefficiency = get_eff_with_booterror(estimator,cost))

table_goodanchor <- res_df_goodanchor %>% filter(natoms == 50) %>% 
  summarise(y = "0.975", pestimate = get_mean_with_booterror(estimator), pcost = get_mean_with_booterror(cost), pfishycost = get_mean_with_booterror(cost_fishyestimation), pvariance = get_var_with_booterror(estimator), pefficiency = get_eff_with_booterror(estimator,cost))

table_comparey <- rbind(table_badanchor, table_goodanchor) %>% setNames(c("y", "estimate", "total cost", "fishy cost",
                                        "variance of estimator", "inefficiency")) %>% select(-`total cost`)
print(table_comparey)
knitr::kable(table_comparey, digits = 0, row.names = NA, format = 'latex', escape = FALSE) %>%
  cat(., file = '../output/binomialssm1d.comparey.tex')

```

Overall we see here a drastic variance reduction when changing $y$.

### Unbiased MCMC tuning

We compute the inefficiency associated with unbiased MCMC with our choice of $k,L,\ell$.

```{r unbiasedmcmcinefficiency}
load("../output/binomialssm1d.uavar.P256.goodanchor.RData")

umcmc_goodanchor <- foreach (irep = 1:nrep, .combine = rbind) %do% {
  res_ <- results.goodx0[[irep]]
  data.frame(rep = c(irep, irep+0.5),
             estimator = c(res_[[1]]$uestimator, res_[[2]]$uestimator),
             cost = c(res_[[1]]$costsignedmeasure, res_[[2]]$costsignedmeasure))
}

umcmc_summary <- umcmc_goodanchor %>% summarise(varestimator = var(estimator), meancost = mean(cost)) %>%
  mutate(inef = varestimator * meancost)
umcmc_summary
```
The inefficiency of unbiased MCMC here is `r round(umcmc_summary$inef, digits = 4)`.
This is to be compared with the estimated value of $v(P,h)$,
around `r round(summary_goodanchorR50$meanestimator, digits = 4)`.
So unbiased MCMC, as tuned here, is pretty close to the standard MCMC
estimator in terms of efficiency.


