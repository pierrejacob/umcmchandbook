---
title: "AR(1) experiments"
output:
  html_document:
    df_print: paged
---

### Files to run

(In this order)

- `ar1_meetingtimes.R`
- `ar1_fishyestimation.R`
- `ar1_avar.R`
- `ar1_longrun.R`
- `ar1_batchmeans.R`
- `ar1_spectralvar.R`

Consider the AR(1) process 
$X_{t}=\phi X_{t-1}+W_{t}$, where $W_{t}\sim\text{Normal}(0,1)$. We set $\phi=0.99$. The initial distribution is $\pi_{0}=\text{Normal}(0,4^{2})$. The target distribution is $\text{Normal}(0,(1-\phi^{2})^{-1})$, and for $h:x\mapsto x$ the asymptotic variance is $v(P,h)=(1-\phi)^{2}=10^4$.



```{r setup, warning=FALSE, message=FALSE}
library(unbiasedpoisson)
library(dplyr)
setmytheme()
## import functions for the AR(1) model
source("ar1_functions.R")
avar <- 1/(1-phi)^2
```


### Coupling and convergence to stationarity

We use a reflection-maximal coupling. Using coupling-based diagnostics (Biswas et al 2019) we find evidence (see below) that the chain converges to stationarity in less than $500$ iterations, and we choose $k=500$, $L=500$, $\ell=2500$ for unbiased MCMC approximations. 


```{r tvupperbounds, message=FALSE, warning=FALSE}
load(file = "../output/ar1.meetingtimes.RData")
meetingtimes <- sapply(meetingtime_runs, function(x) x$meetingtime)
niterations <- 1000
ubounds <- sapply(1:niterations, function(t) tv_upper_bound(unlist(meetingtimes), lag, t))
g_tvbounds <- qplot(x = 1:niterations, y = ubounds, geom = "line") +
  ylab("TV distance") + xlab("iteration")
g_tvbounds <- g_tvbounds + scale_y_log10(breaks = c(0, 0.001, 0.01, 0.1,1)) + geom_rangeframe() 
g_tvbounds
ggsave(filename = "../output/ar1.tvupperbounds.pdf", plot = g_tvbounds, width = 8, height = 5)
```

### Fishy function estimation

The state $y\in\mathbb{X}=\mathbb{R}$, used to define the fishy function $g_{y}$ and its estimator $G_y$, is set as $y=0$.
We estimate the fishy function evaluations $g_{y}(x)$ for a grid of values of $x$. We plot the estimated fishy function, and the estimated second moment of $G_y(x)$ as a function of $x$. 

```{r printfishy, message=FALSE, warning=FALSE}
load(file = "../output/ar1.fishyfunction.RData")
ghtilde <- ggplot(df, aes(x = x, y = estimator)) + geom_hline(yintercept = 0, alpha = 0.2) +
  geom_smooth(colour = "#005BBB", formula = y ~ s(x, bs = "cs"), method = "gam") + xlab('x') + ylab(TeX("fishy function(x)$")) 
ghtilde <- ghtilde + geom_abline(intercept = 0, slope = 1/(1-phi), linetype = 2, colour = 'white')
ghtilde
ggsave(filename = "../output/ar1.htilde.pdf", plot = ghtilde, width = 8, height = 5)
```


```{r print2ndmoment, message=FALSE, warning=FALSE}
ghtildesquare <- ggplot(df, aes(x = x, y = estimator^2)) + geom_hline(yintercept = 0, alpha = 0.2) + geom_smooth(colour = "#005BBB", formula = y ~ s(x, bs = "cs"), method = "gam") + xlab('x') + ylab(TeX("second moment (x)$")) 
ghtildesquare
ggsave(filename = "../output/ar1.Htilde.m2.pdf", plot = ghtildesquare, width = 8, height = 5)
```

The above plots are obtained with `r nrep` estimate per $x$ value.

### Performance of proposed estimator

The performance of the proposed estimator of $v(P,h)$ is shown in the table below.

```{r efficiencyRtable_alt, message=FALSE, warning=FALSE}
load("../output/ar1.uavar.RData")
# print mean
get_mean_with_booterror <- function(v, nb = 1000){
  bootresult <- boot::boot(data = v, statistic = function(v,indices) mean(v[indices]), R = nb)
  paste0("[", 
         paste0(prettyNum(as.numeric(quantile(bootresult$t, probs = c(0.025, 0.975))), digits=1, scientific = F, nsmall = 0), collapse = " - "),
         "]")
}
# print variance
get_var_with_booterror <- function(v, nb = 1000){
  bootresult <- boot::boot(data = v, statistic = function(v,indices) var(v[indices]), R = nb)
  paste0("[", 
         paste0(prettyNum(as.numeric(quantile(bootresult$t, probs = c(0.025, 0.975))), digits=2, scientific = T), collapse = " - "),
         "]")
}
# print efficiency
get_eff_with_booterror <- function(v, c, nb = 1000){
  bootresult <- boot::boot(data = cbind(v,c), statistic = function(x,indices) var(x[indices,1])*mean(x[indices,2]), R = nb)
  paste0("[", 
         paste0(prettyNum(as.numeric(quantile(bootresult$t, probs = c(0.025, 0.975))), digits=2, scientific = T), collapse = " - "),
         "]")
}

table_alt <- results %>% ungroup() %>% group_by(natoms) %>% summarise(pestimate = get_mean_with_booterror(estimator), pcost = get_mean_with_booterror(cost), pfishycost = get_mean_with_booterror(cost_fishyestimation), pvariance = get_var_with_booterror(estimator), pefficiency = get_eff_with_booterror(estimator,cost))
table_alt <- table_alt %>% setNames(c("R", "estimate", "cost", "fishy cost", 
                              "variance of estimator", "inefficiency"))
print(table_alt, width = Inf)
knitr::kable(table_alt, digits = 0, row.names = NA, format = 'latex', escape = FALSE) %>%
  cat(., file = '../output/ar1.summary.alt.tex')
```

That table is obtained using `r nrep` independent replicates. From the estimates we obtain confidence intervals for each quantity in the table using the nonparametric bootstrap (i.e. sample with replacement and then compute aggregate quantity). The columns are:

- estimate: proposed estimate of the $v(P,h)$. Here the estimand $v(P,h)$ is equal to $10^{4}$.
- cost: total cost of each proposed estimate, in units of "Markov transitions" 
- fishy cost: subcost associated with the fishy estimates
- variance of estimator: empirical variance of the estimates
- inefficiency: product of the variance and the total cost (the goal is for that quantity to be as small as possible)

We see that the variance decreases with $R$ but the cost increases. The inefficiency seems to "stabilize" as $R$ gets bigger. An important take-away is that $R=1$ is not a good choice. 

As a rule of thumb we propose to choose $R$ such that "fishy cost" is around $50\%$ of the total cost: this way we waste at most half of our computing time.

### Comparison with other methods

To make the comparison simpler, we focus on our estimator obtained with $R=50$.
```{r tableR=50}
summaryR50 <- results %>% ungroup() %>% filter(natoms == 50) %>% summarise(meancost = mean(cost), variance = var(estimator))
meancost_ <- summaryR50$meancost
variance_ <- summaryR50$variance
print(prettyNum(meancost_, scientific=T, digits = 3))
print(prettyNum(variance_, scientific=T, digits = 3))
```

The expected cost and the variance characterize the performance of the proposed estimator.
Next we will compare these metrics with various alternatives.

#### Naive parallelization

We can run $M$ parallel chains, each of length $T$ (post-burnin), 
compute an ergodic average for each chain, and report the empirical variance.
This is easy to parallelize over the $M$ runs. 

Below we compute exactly that variance for various $T$ (and $M\to\infty$)
which allows to see the bias of this strategy, for $T$ on the order of $10^4$,
which is roughly the cost of the proposed estimator (see table above).

```{r curvevariance_of_average}
sigma2 = 1.
sigma2var = sigma2/(1.-phi^2)
## exact formula
## see: https://stats.stackexchange.com/questions/90062/variance-of-sample-mean-in-an-ar1-process
f <- function(timehorizon) sigma2var / (timehorizon^2) * (timehorizon + 2 * ((timehorizon-1)/phi - timehorizon + phi^(timehorizon-1))/((1./phi - 1)^2))
curve(x * f(x), from = 2000, to = 20000, xlab = "time horizon", ylab = "variance of average",
      ylim = c(9500, 10100))
abline(h = 1e4, lty = 2, lwd = 2)
```

From that we see that the naive strategy underestimates the actual variance (horizontal dashed line), for time horizons that lead to costs per run that are comparable to our proposed estimators. We select $T=13000$ (to match our proposed estimator with $R=50$) and compute the associated bias:
```{r biasnaive}
timehorizon <- 13000
bias <- timehorizon*f(timehorizon) - avar
print(bias)
```

That bias is not negligible but not very large either ($<1\%$ of $v(P,h)$).

We next look at the variance associated with this naive strategy.
We generate $M=1000$ copies and compute the variance of the empirical variance.

```{r naiveparallel, message=FALSE, warning=FALSE}
library(doParallel)
library(doRNG)
registerDoParallel(cores = detectCores()-2)
set.seed(1)
## run independent chains
nrep <- 1000
library(dplyr)
timehorizon <- 13000
runs <- foreach(irep = 1:nrep, .combine = c) %dorng% {
  chainhistory <- rep(0, timehorizon)
  state <- list(position = rnorm(1, 0, sqrt(sigma2var))) # start at stationarity
  for (timeindex in 1:timehorizon){
    state <- single_kernel(state)
    chainhistory[timeindex] <- state$position
  }
  mean(chainhistory)
}
bootresult <- boot::boot(data = runs, statistic = function(v, indices) timehorizon*var(v[indices]), R = 1000)
bootvar_samplevar <- var(bootresult$t)[1,1]
prettyNum(bootvar_samplevar*nrep, scientific=T, digits=3)
var_samplevar <- (mean((runs - mean(runs))^4) - var(runs)^2 *(nrep-3)/(nrep-1))/nrep
prettyNum(var_samplevar*timehorizon^2*nrep, scientific=T, digits=3)
```

We compare these values to `r prettyNum(variance_, scientific=T, digits = 3)`,
from which we see that the naive strategy has a larger variance than our proposed estimators.

#### Batch means

Let's try to compare batch means and the proposed method.
The following plot shows the MSE of different batch means estimates
as a function of cost. Each batch mean estimate employs $1$ or more chains,
and a lugsail parameter $r\in\{1,2,3\}$, as implemented in the `mcmcse` package.

For the proposed estimator, we know that the cost of each estimator is 
`r prettyNum(meancost_, scientific=T, digits = 3)` and its 
variance is `r prettyNum(variance_, scientific=T, digits = 3)`, so 
we compute the MSE by dividing the variance by the number of samples that we
can obtain in a given computing budget. We plot that MSE with a full line.


```{r plotbm, message=FALSE, warning=FALSE}
load("../output/ar1.uavar.RData")
table <- results %>% group_by(natoms) %>% summarise( estimate = mean(estimator),
                                           totalcost = mean(cost),
                                           fishycost = mean(cost_fishyestimation),
                                           variance = var(estimator))
load(file = "../output/ar1batchmeans.RData")
plot.df <- bm.df %>% group_by(r, nchains, timehorizon) %>% 
  summarise(mean_estimate = mean(varestimate), 
            cost = mean(nchains) * mean(timehorizon),
            bias = mean(varestimate) - avar,
            mse = mean((varestimate - avar)^2))
plot.df$config <- paste0("#", plot.df$nchains, "r", plot.df$r)
plot.df <- plot.df %>% ungroup() %>% select(r, nchains, cost, mse, config)
g <- ggplot(plot.df, aes(x = cost, y = mse, colour = factor(r), shape = factor(nchains))) + geom_point(size = 5)
g <- g + scale_x_log10() + scale_y_log10()
proposed.df <- foreach(nat = table$natoms, .combine = rbind) %do% {
  proposed_cost <- table %>% filter(natoms == nat) %>% pull(totalcost)
  proposed_var <- table %>% filter(natoms == nat) %>% pull(variance)
  df_ <- data.frame(cost = 10^(seq(from = 4, to = 7, by = 0.25)))
  df_$natoms = nat
  df_$mse <- proposed_var / floor(df_$cost/proposed_cost)
  df_ %>% select(natoms, cost, mse)
}
g <- g + geom_line(data=proposed.df %>% filter(natoms == 50), aes(linetype = factor(natoms), colour = NULL, shape = NULL))
g <- g + xlab("total cost") + ylab("MSE") + scale_color_grey(name = "BM: r") + scale_shape(name="BM: # chains") + scale_linetype(name="proposed method (R=50)", labels = NULL)
g <- g + guides(colour = guide_legend(override.aes = list(size = 3, shape = 15, linetype = 0))) + theme(legend.box = "vertical")
g
ggsave(filename = "../output/ar1.bmcomparison.pdf", plot = g, width = 8, height = 5)
```

The overall message is that our proposed estimates deliver a smaller MSE
as a function of cost, at least for large enough computing budget. Also,
again it's not good to use $R=1$ here.


We also create a plot about the bias of batch means estimators.

```{r plotbmbias, message=FALSE, warning=FALSE}
plot.df <- bm.df %>% group_by(r, nchains, timehorizon) %>% 
  summarise(mean_estimate = mean(varestimate), 
            cost = mean(nchains) * mean(timehorizon),
            bias = mean(varestimate) - avar,
            mse = mean((varestimate - avar)^2))
plot.df$config <- paste0("#", plot.df$nchains, "r", plot.df$r)
plot.df <- plot.df %>% ungroup() %>% select(r, nchains, cost, bias, config)
g <- ggplot(plot.df, aes(x = cost, y = bias, colour = factor(r), shape = factor(nchains))) + geom_point(size = 5)
g <- g + scale_x_log10(limits = c(9e3, 1.1e7)) + ylim(-2500,+2500)
g <- g + xlab("total cost") + ylab("bias") + scale_color_grey(name = "BM: r") + scale_shape(name="BM: # chains")
g <- g + guides(colour = guide_legend(override.aes = list(size = 3, shape = 15, linetype = 0))) + theme(legend.box = "vertical")
g
ggsave(filename = "../output/ar1.bmbias.pdf", plot = g, width = 8, height = 5)
```

We see that the 'r' parameter has a huge effect on the bias.


#### Spectral variance

We can do the same with spectral variance estimates.

```{r plotsm, message=FALSE, warning=FALSE}
load(file = "../output/ar1spectralvar.RData")
plot.df <- spectral.df %>% filter(nchains >= 1) %>% group_by(r, nchains, timehorizon) %>% 
  summarise(mean_estimate = mean(varestimate), 
            cost = mean(nchains) * mean(timehorizon),
            bias = mean(varestimate) - avar,
            mse = mean((varestimate - avar)^2))
plot.df$config <- paste0("#", plot.df$nchains, "r", plot.df$r)
plot.df <- plot.df %>% ungroup() %>% select(r, nchains, cost, mse, config)
g <- ggplot(plot.df, aes(x = cost, y = mse, colour = factor(r), shape = factor(nchains))) + geom_point(size = 5)
g <- g + scale_x_log10() + scale_y_log10()
proposed.df <- foreach(nat = table$natoms, .combine = rbind) %do% {
  proposed_cost <- table %>% filter(natoms == nat) %>% pull(totalcost)
  proposed_var <- table %>% filter(natoms == nat) %>% pull(variance)
  df_ <- data.frame(cost = 10^(seq(from = 4, to = 7, by = 0.25)))
  df_$natoms = nat
  df_$mse <- proposed_var / floor(df_$cost/proposed_cost)
  df_ %>% select(natoms, cost, mse)
}
g <- g + geom_line(data=proposed.df %>% filter(natoms == 50), aes(linetype = factor(natoms), colour = NULL, shape = NULL))
g <- g + xlab("total cost") + ylab("MSE") + scale_color_grey(name = "SV: r") + scale_shape(name="SV: # chains") + scale_linetype(name="proposed method (R=50)", labels=NULL)
g <- g + guides(colour = guide_legend(override.aes = list(size = 3, shape = 15, linetype = 0))) + theme(legend.box = "vertical")
g
ggsave(filename = "../output/ar1.spectralcomparison.pdf", plot = g, width = 8, height = 5)
```

We see that spectral variance estimates are better than batch means,
but our proposed estimates remain competitive.


```{r plotspectralbias, message=FALSE, warning=FALSE}
plot.df <- spectral.df %>% filter(nchains >= 1) %>% group_by(r, nchains, timehorizon) %>% 
  summarise(mean_estimate = mean(varestimate), 
            cost = mean(nchains) * mean(timehorizon),
            bias = mean(varestimate) - avar,
            mse = mean((varestimate - avar)^2))
plot.df$config <- paste0("#", plot.df$nchains, "r", plot.df$r)
plot.df <- plot.df %>% ungroup() %>% select(r, nchains, cost, bias, config)
g <- ggplot(plot.df, aes(x = cost, y = bias, colour = factor(r), shape = factor(nchains))) + geom_point(size = 5)
g <- g + scale_x_log10(limits = c(9e3, 1.1e7)) + ylim(-2500,+2500)
g <- g + xlab("total cost") + ylab("bias") + scale_color_grey(name = "SV: r") + scale_shape(name="SV: # chains")
g <- g + guides(colour = guide_legend(override.aes = list(size = 3, shape = 15, linetype = 0))) + theme(legend.box = "vertical")
g
ggsave(filename = "../output/ar1.spectralbias.pdf", plot = g, width = 8, height = 5)
```

